{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"Blazingly-fast Bayesian Deep Learning in PyTorch."},{"location":"#setup","title":"Setup","text":"<p>You can install inferno using <code>pip</code>:</p> <pre><code>pip install inferno-torch\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import inferno\n</code></pre>"},{"location":"CONTRIBUTING/","title":"Contribution Guidelines","text":""},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with contributions to the library follow these steps:</p> <ol> <li>Fork the repository.</li> <li>Clone your fork to your machine:     <pre><code>git clone git@github.com:my-username/inferno.git\n</code></pre></li> <li>Install the Python package in an editable fashion with all development dependencies (in a new virtual environment):     <pre><code>cd inferno\npip install -e .[dev]\n</code></pre></li> </ol> <p>That's it! To make contributions to the main library simply push your changes to your fork on GitHub and create a pull request.</p>"},{"location":"CONTRIBUTING/#continuous-integration","title":"Continuous Integration","text":"<p>We use <code>tox</code> to simplify any tasks related to continuous integration.</p>"},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<p>You can run (a subset of) tests via <pre><code>tox -e py3 -- path-to-tests\n</code></pre></p>"},{"location":"CONTRIBUTING/#formatting-code","title":"Formatting Code","text":"<p>To ensure consistent formatting throughout the library, we check for properly formatted code and imports. </p> <p>To check whether your code is properly formatted run:</p> <pre><code>tox -e format -- . --check --diff\n</code></pre> <p>If you want <code>tox</code> to automatically format a specific folder, simply run <pre><code>tox -e format -- folder-to-format\n</code></pre></p>"},{"location":"CONTRIBUTING/#building-the-documentation","title":"Building the Documentation","text":"<p>You can build the documentation and view it locally in your browser.</p> <pre><code>tox -e docs -- serve \n</code></pre> <p>Now open a browser and enter the local address shown in your terminal.</p>"},{"location":"CONTRIBUTING/#running-code-examples","title":"Running Code Examples","text":"<p>You can run the code examples in the documentation, like so.</p> <pre><code># Run all examples\ntox -e examples\n\n# Run a specific example\ntox -e examples -- docs/examples/my_example/run.py \n</code></pre>"},{"location":"api/datasets/","title":"<code class=\"doc-symbol doc-symbol-heading\">datasets</code>","text":""},{"location":"api/datasets/#inferno.datasets","title":"datasets","text":"<p>Classes:</p> Name Description <code>MNISTC</code> <p>Corrupted MNIST image classification dataset.</p> <code>CIFAR10C</code> <p>Corrupted CIFAR10 image classification dataset.</p> <code>CIFAR100C</code> <p>Corrupted CIFAR100 image classification dataset.</p> <code>TinyImageNet</code> <p>TinyImageNet image classification dataset.</p> <code>TinyImageNetC</code> <p>Corrupted TinyImageNet image classification dataset.</p>"},{"location":"api/datasets/#inferno.datasets.MNISTC","title":"MNISTC","text":"<pre><code>MNISTC(\n    root: Path | str = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    corruptions: list[str] = [\n        \"brightness\",\n        \"canny_edges\",\n        \"dotted_line\",\n        \"fog\",\n        \"glass_blur\",\n        \"impulse_noise\",\n        \"motion_blur\",\n        \"rotate\",\n        \"scale\",\n        \"shear\",\n        \"shot_noise\",\n        \"spatter\",\n        \"stripe\",\n        \"translate\",\n        \"zigzag\",\n    ],\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>VisionDataset</code></p> <p>Corrupted MNIST image classification dataset.</p> <p>Contains 10,000 test images for each one of 15 corruptions. From MNIST-C: A Robustness Benchmark for Computer Vision.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Path | str</code> <p>Root directory of the dataset.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to the data.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Transform to apply to the targets.</p> <code>None</code> <code>corruptions</code> <code>list[str]</code> <p>List of corruptions to apply to the data.</p> <code>['brightness', 'canny_edges', 'dotted_line', 'fog', 'glass_blur', 'impulse_noise', 'motion_blur', 'rotate', 'scale', 'shear', 'shot_noise', 'spatter', 'stripe', 'translate', 'zigzag']</code> <code>download</code> <code>bool</code> <p>If true, downloads the dataset from the internet and puts it in the root directory.</p> <code>False</code> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>base_folder</code> <code>corruptions</code> <code>data</code> <code>filename</code> <code>sub_folder</code> <code>targets</code> <code>url</code> <code>zip_md5</code>"},{"location":"api/datasets/#inferno.datasets.MNISTC.base_folder","title":"base_folder","text":"<pre><code>base_folder = Path('MNIST-C/raw')\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.MNISTC.corruptions","title":"corruptions","text":"<pre><code>corruptions = corruptions\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.MNISTC.data","title":"data","text":"<pre><code>data = concatenate(\n    [\n        load(\n            root\n            / sub_folder\n            / corruption\n            / \"test_images.npy\"\n        )\n        for corruption in corruptions\n    ],\n    axis=0,\n)\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.MNISTC.filename","title":"filename","text":"<pre><code>filename = 'mnist_c.zip'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.MNISTC.sub_folder","title":"sub_folder","text":"<pre><code>sub_folder = Path('mnist_c')\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.MNISTC.targets","title":"targets","text":"<pre><code>targets = astype(int64)\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.MNISTC.url","title":"url","text":"<pre><code>url = 'https://zenodo.org/record/3239543/files/mnist_c.zip'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.MNISTC.zip_md5","title":"zip_md5","text":"<pre><code>zip_md5 = '4b34b33045869ee6d424616cd3a65da3'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.MNISTC.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C","title":"CIFAR10C","text":"<pre><code>CIFAR10C(\n    root: Path | str = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    corruptions: list[str] = [\n        \"brightness\",\n        \"contrast\",\n        \"defocus_blur\",\n        \"elastic_transform\",\n        \"fog\",\n        \"frost\",\n        \"gaussian_blur\",\n        \"gaussian_noise\",\n        \"glass_blur\",\n        \"impulse_noise\",\n        \"jpeg_compression\",\n        \"motion_blur\",\n        \"pixelate\",\n        \"saturate\",\n        \"shot_noise\",\n        \"snow\",\n        \"spatter\",\n        \"speckle_noise\",\n        \"zoom_blur\",\n    ],\n    shift_severity: int = 5,\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>VisionDataset</code></p> <p>Corrupted CIFAR10 image classification dataset.</p> <p>Contains 10,000 test images for each corruption. From Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Path | str</code> <p>Root directory of the dataset.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to the data.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Transform to apply to the targets.</p> <code>None</code> <code>corruptions</code> <code>list[str]</code> <p>List of corruptions to apply to the data.</p> <code>['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'jpeg_compression', 'motion_blur', 'pixelate', 'saturate', 'shot_noise', 'snow', 'spatter', 'speckle_noise', 'zoom_blur']</code> <code>shift_severity</code> <code>int</code> <p>Severity of the corruption to apply. Must be an integer between 1 and 5.</p> <code>5</code> <code>download</code> <code>bool</code> <p>If true, downloads the dataset from the internet and puts it in the root directory.</p> <code>False</code> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>base_folder</code> <code>corruption_data_checksums</code> <code>corruptions</code> <code>data</code> <code>filename</code> <code>shift_severity</code> <code>sub_folder</code> <code>targets</code> <code>tgz_md5</code> <code>url</code>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.base_folder","title":"base_folder","text":"<pre><code>base_folder = Path('CIFAR10-C/raw')\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.corruption_data_checksums","title":"corruption_data_checksums","text":"<pre><code>corruption_data_checksums = {\n    \"fog\": \"7b397314b5670f825465fbcd1f6e9ccd\",\n    \"jpeg_compression\": \"2b9cc4c864e0193bb64db8d7728f8187\",\n    \"zoom_blur\": \"6ea8e63f1c5cdee1517533840641641b\",\n    \"speckle_noise\": \"ef00b87611792b00df09c0b0237a1e30\",\n    \"glass_blur\": \"7361fb4019269e02dbf6925f083e8629\",\n    \"spatter\": \"8a5a3903a7f8f65b59501a6093b4311e\",\n    \"shot_noise\": \"3a7239bb118894f013d9bf1984be7f11\",\n    \"defocus_blur\": \"7d1322666342a0702b1957e92f6254bc\",\n    \"elastic_transform\": \"9421657c6cd452429cf6ce96cc412b5f\",\n    \"gaussian_blur\": \"c33370155bc9b055fb4a89113d3c559d\",\n    \"frost\": \"31f6ab3bce1d9934abfb0cc13656f141\",\n    \"saturate\": \"1cfae0964219c5102abbb883e538cc56\",\n    \"brightness\": \"0a81ef75e0b523c3383219c330a85d48\",\n    \"snow\": \"bb238de8555123da9c282dea23bd6e55\",\n    \"gaussian_noise\": \"ecaf8b9a2399ffeda7680934c33405fd\",\n    \"motion_blur\": \"fffa5f852ff7ad299cfe8a7643f090f4\",\n    \"contrast\": \"3c8262171c51307f916c30a3308235a8\",\n    \"impulse_noise\": \"2090e01c83519ec51427e65116af6b1a\",\n    \"labels\": \"c439b113295ed5254878798ffe28fd54\",\n    \"pixelate\": \"0f14f7e2db14288304e1de10df16832f\",\n}\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.corruptions","title":"corruptions","text":"<pre><code>corruptions = corruptions\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.data","title":"data","text":"<pre><code>data = concatenate(\n    [\n        load(root / sub_folder / corruption + \".npy\")[\n            shift_severity\n            - 1 * 10000 : shift_severity * 10000\n        ]\n        for corruption in corruptions\n    ],\n    axis=0,\n)\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.filename","title":"filename","text":"<pre><code>filename = 'CIFAR-10-C.tar'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.shift_severity","title":"shift_severity","text":"<pre><code>shift_severity = shift_severity\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.sub_folder","title":"sub_folder","text":"<pre><code>sub_folder = Path('CIFAR-10-C')\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.targets","title":"targets","text":"<pre><code>targets = astype(int64)\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.tgz_md5","title":"tgz_md5","text":"<pre><code>tgz_md5 = '56bf5dcef84df0e2308c6dcbcbbd8499'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.url","title":"url","text":"<pre><code>url = (\n    \"https://zenodo.org/record/2535967/files/CIFAR-10-C.tar\"\n)\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR10C.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C","title":"CIFAR100C","text":"<pre><code>CIFAR100C(\n    root: Path | str = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    corruptions: list[str] = [\n        \"brightness\",\n        \"contrast\",\n        \"defocus_blur\",\n        \"elastic_transform\",\n        \"fog\",\n        \"frost\",\n        \"gaussian_blur\",\n        \"gaussian_noise\",\n        \"glass_blur\",\n        \"impulse_noise\",\n        \"jpeg_compression\",\n        \"motion_blur\",\n        \"pixelate\",\n        \"saturate\",\n        \"shot_noise\",\n        \"snow\",\n        \"spatter\",\n        \"speckle_noise\",\n        \"zoom_blur\",\n    ],\n    shift_severity: int = 5,\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>CIFAR10C</code></p> <p>Corrupted CIFAR100 image classification dataset.</p> <p>From Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Path | str</code> <p>Root directory of the dataset.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to the data.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Transform to apply to the targets.</p> <code>None</code> <code>corruptions</code> <code>list[str]</code> <p>List of corruptions to apply to the data.</p> <code>['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'jpeg_compression', 'motion_blur', 'pixelate', 'saturate', 'shot_noise', 'snow', 'spatter', 'speckle_noise', 'zoom_blur']</code> <code>shift_severity</code> <code>int</code> <p>Severity of the corruption to apply. Must be an integer between 1 and 5.</p> <code>5</code> <code>download</code> <code>bool</code> <p>If true, downloads the dataset from the internet and puts it in root directory.</p> <code>False</code> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>base_folder</code> <code>corruption_data_checksums</code> <code>corruptions</code> <code>data</code> <code>filename</code> <code>shift_severity</code> <code>sub_folder</code> <code>targets</code> <code>tgz_md5</code> <code>url</code>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.base_folder","title":"base_folder","text":"<pre><code>base_folder = Path('CIFAR100-C/raw')\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.corruption_data_checksums","title":"corruption_data_checksums","text":"<pre><code>corruption_data_checksums = {\n    \"fog\": \"4efc7ebd5e82b028bdbe13048e3ea564\",\n    \"jpeg_compression\": \"c851b7f1324e1d2ffddeb76920576d11\",\n    \"zoom_blur\": \"0204613400c034a81c4830d5df81cb82\",\n    \"speckle_noise\": \"e3f215b1a0f9fd9fd6f0d1cf94a7ce99\",\n    \"glass_blur\": \"0bf384f38e5ccbf8dd479d9059b913e1\",\n    \"spatter\": \"12ccf41d62564d36e1f6a6ada5022728\",\n    \"shot_noise\": \"b0a1fa6e1e465a747c1b204b1914048a\",\n    \"defocus_blur\": \"d923e3d9c585a27f0956e2f2ad832564\",\n    \"elastic_transform\": \"a0792bd6581f6810878be71acedfc65a\",\n    \"gaussian_blur\": \"5204ba0d557839772ef5a4196a052c3e\",\n    \"frost\": \"3a39c6823bdfaa0bf8b12fe7004b8117\",\n    \"saturate\": \"c0697e9fdd646916a61e9c312c77bf6b\",\n    \"brightness\": \"f22d7195aecd6abb541e27fca230c171\",\n    \"snow\": \"0237be164583af146b7b144e73b43465\",\n    \"gaussian_noise\": \"ecc4d366eac432bdf25c024086f5e97d\",\n    \"motion_blur\": \"732a7e2e54152ff97c742d4c388c5516\",\n    \"contrast\": \"322bb385f1d05154ee197ca16535f71e\",\n    \"impulse_noise\": \"3b3c210ddfa0b5cb918ff4537a429fef\",\n    \"labels\": \"bb4026e9ce52996b95f439544568cdb2\",\n    \"pixelate\": \"96c00c60f144539e14cffb02ddbd0640\",\n}\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.corruptions","title":"corruptions","text":"<pre><code>corruptions = corruptions\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.data","title":"data","text":"<pre><code>data = concatenate(\n    [\n        load(root / sub_folder / corruption + \".npy\")[\n            shift_severity\n            - 1 * 10000 : shift_severity * 10000\n        ]\n        for corruption in corruptions\n    ],\n    axis=0,\n)\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.filename","title":"filename","text":"<pre><code>filename = 'CIFAR-100-C.tar'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.shift_severity","title":"shift_severity","text":"<pre><code>shift_severity = shift_severity\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.sub_folder","title":"sub_folder","text":"<pre><code>sub_folder = Path('CIFAR-100-C')\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.targets","title":"targets","text":"<pre><code>targets = astype(int64)\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.tgz_md5","title":"tgz_md5","text":"<pre><code>tgz_md5 = '11f0ed0f1191edbf9fa23466ae6021d3'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.url","title":"url","text":"<pre><code>url = \"https://zenodo.org/record/3555552/files/CIFAR-100-C.tar\"\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.CIFAR100C.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNet","title":"TinyImageNet","text":"<pre><code>TinyImageNet(\n    root: str | Path,\n    train: bool = True,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>VisionDataset</code></p> <p>TinyImageNet image classification dataset.</p> <p>The training dataset contains 100,000 images of 200 classes (500 for each class) downsized to 64x64 color images. The test set has 10,000 images (50 for each class).</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str | Path</code> <p>Root directory of the dataset.</p> required <code>train</code> <code>bool</code> <p>If True, creates dataset from training data, otherwise from test data.</p> <code>True</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to the data.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Transform to apply to the targets.</p> <code>None</code> <code>download</code> <code>bool</code> <p>If true, downloads the dataset from the internet and puts it in the root directory.</p> <code>False</code> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>base_folder</code> <code>class_to_idx</code> <code>data</code> <code>filename</code> <code>targets</code> <code>tgz_md5</code> <code>url</code>"},{"location":"api/datasets/#inferno.datasets.TinyImageNet.base_folder","title":"base_folder","text":"<pre><code>base_folder = Path('TinyImageNet/raw')\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNet.class_to_idx","title":"class_to_idx","text":"<pre><code>class_to_idx = {classes[i]: _7for i in range(len(classes))}\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNet.data","title":"data","text":"<pre><code>data = []\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNet.filename","title":"filename","text":"<pre><code>filename = 'tiny-imagenet-200.zip'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNet.targets","title":"targets","text":"<pre><code>targets = [item[1] for item in data]\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNet.tgz_md5","title":"tgz_md5","text":"<pre><code>tgz_md5 = '90528d7ca1a48142e341f4ef8d21d0de'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNet.url","title":"url","text":"<pre><code>url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNet.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNetC","title":"TinyImageNetC","text":"<pre><code>TinyImageNetC(\n    root: Path | str = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    corruptions: list[str] = [\n        \"brightness\",\n        \"contrast\",\n        \"defocus_blur\",\n        \"elastic_transform\",\n        \"fog\",\n        \"frost\",\n        \"gaussian_blur\",\n        \"gaussian_noise\",\n        \"glass_blur\",\n        \"impulse_noise\",\n        \"jpeg_compression\",\n        \"motion_blur\",\n        \"pixelate\",\n        \"saturate\",\n        \"shot_noise\",\n        \"snow\",\n        \"spatter\",\n        \"speckle_noise\",\n        \"zoom_blur\",\n    ],\n    shift_severity: int = 5,\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>VisionDataset</code></p> <p>Corrupted TinyImageNet image classification dataset.</p> <p>Contains 10,000 64x64 color test images for each corruption (200 classes, 50 images per class). From Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Path | str</code> <p>Root directory of the dataset.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to the data.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Transform to apply to the targets.</p> <code>None</code> <code>corruptions</code> <code>list[str]</code> <p>List of corruptions to apply to the data.</p> <code>['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'jpeg_compression', 'motion_blur', 'pixelate', 'saturate', 'shot_noise', 'snow', 'spatter', 'speckle_noise', 'zoom_blur']</code> <code>shift_severity</code> <code>int</code> <p>Severity of the corruption to apply. Must be an integer between 1 and 5.</p> <code>5</code> <code>download</code> <code>bool</code> <p>If true, downloads the dataset from the internet and puts it in the root directory.</p> <code>False</code> <p>Methods:</p> Name Description <code>download</code> <p>Download the dataset.</p> <p>Attributes:</p> Name Type Description <code>base_folder</code> <code>corruptions</code> <code>data</code> <code>filename</code> <code>shift_severity</code> <code>targets</code> <code>tgz_md5</code> <code>url</code>"},{"location":"api/datasets/#inferno.datasets.TinyImageNetC.base_folder","title":"base_folder","text":"<pre><code>base_folder = Path('TinyImageNet-C/raw')\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNetC.corruptions","title":"corruptions","text":"<pre><code>corruptions = corruptions\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNetC.data","title":"data","text":"<pre><code>data = []\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNetC.filename","title":"filename","text":"<pre><code>filename = [\n    \"Tiny-ImageNet-C.tar\",\n    \"Tiny-ImageNet-C-extra.tar\",\n]\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNetC.shift_severity","title":"shift_severity","text":"<pre><code>shift_severity = shift_severity\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNetC.targets","title":"targets","text":"<pre><code>targets = []\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNetC.tgz_md5","title":"tgz_md5","text":"<pre><code>tgz_md5 = [\n    \"f9c9a9dbdc11469f0b850190f7ad8be1\",\n    \"0db0588d243cf403ef93449ec52b70eb\",\n]\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNetC.url","title":"url","text":"<pre><code>url = 'https://zenodo.org/record/8206060/files/'\n</code></pre>"},{"location":"api/datasets/#inferno.datasets.TinyImageNetC.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre> <p>Download the dataset.</p>"},{"location":"api/loss_fns/","title":"<code class=\"doc-symbol doc-symbol-heading\">loss_fns</code>","text":""},{"location":"api/loss_fns/#inferno.loss_fns","title":"loss_fns","text":"<p>Loss functions.</p> <p>Classes:</p> Name Description <code>VariationalFreeEnergy</code> <p>Variational Free Energy Loss.</p> <p>Attributes:</p> Name Type Description <code>NegativeELBO</code>"},{"location":"api/loss_fns/#inferno.loss_fns.NegativeELBO","title":"NegativeELBO","text":"<pre><code>NegativeELBO = VariationalFreeEnergy\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy","title":"VariationalFreeEnergy","text":"<pre><code>VariationalFreeEnergy(\n    nll: _Loss,\n    model: BNNModule,\n    prior_loc: Float[Tensor, \"parameter\"] | None = None,\n    prior_scale: Float[Tensor, \"parameter\"] | None = None,\n    kl_weight: float | None = 1.0,\n    reduction: str = \"mean\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Variational Free Energy Loss.</p> <p>Computes the variational free energy loss for variational inference with the Kullback-Leibler regularization term computed in weight space. This is also known as the negative evidence lower bound (ELBO).</p> <p>Parameters:</p> Name Type Description Default <code>nll</code> <code>_Loss</code> <p>Loss function defining the negative log-likelihood.</p> required <code>model</code> <code>BNNModule</code> <p>The probabilistic model.</p> required <code>prior_loc</code> <code>Float[Tensor, 'parameter'] | None</code> <p>Location(s) of the prior Gaussian distribution.</p> <code>None</code> <code>prior_scale</code> <code>Float[Tensor, 'parameter'] | None</code> <p>Scale(s) of the prior Gaussian distribution.</p> <code>None</code> <code>kl_weight</code> <code>float | None</code> <p>Weight for the KL divergence term. If <code>None</code>, chooses the weight inversely proportional to the number of mean parameters.</p> <code>1.0</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply to the output: <code>``'mean'</code> | <code>'sum'</code>. <code>'mean'</code>: the weighted mean of the output is taken, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>kl_weight</code> <code>model</code> <code>nll</code> <code>numel_mean_parameters</code> <code>prior_loc</code> <code>prior_scale</code> <code>reduction</code>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.kl_weight","title":"kl_weight","text":"<pre><code>kl_weight = kl_weight\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.model","title":"model","text":"<pre><code>model = model\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.nll","title":"nll","text":"<pre><code>nll = nll\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.numel_mean_parameters","title":"numel_mean_parameters","text":"<pre><code>numel_mean_parameters = sum(\n    numel()\n    for (name, param) in named_parameters()\n    if requires_grad\n    and \"params.\" in name\n    and \"cov.\" not in name\n)\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.prior_loc","title":"prior_loc","text":"<pre><code>prior_loc = prior_loc\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.prior_scale","title":"prior_scale","text":"<pre><code>prior_scale = prior_scale\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.reduction","title":"reduction","text":"<pre><code>reduction = reduction\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch in_feature\"],\n    target: Float[Tensor, \"batch out_feature\"],\n) -&gt; Float[Tensor, \"\"]\n</code></pre>"},{"location":"api/models/","title":"<code class=\"doc-symbol doc-symbol-heading\">models</code>","text":""},{"location":"api/models/#inferno.models","title":"models","text":"<p>Pre-defined models.</p> <p>Classes:</p> Name Description <code>Ensemble</code> <p>An ensemble of models.</p> <code>LeNet5</code> <p>A simple convolutional neural network for image classification of 28x28 grayscale images.</p> <code>MLP</code> <p>A fully-connected feedforward neural network with the same activation function in each layer.</p> <code>ResNeXt101_32X8D</code> <p>ResNext-101 (32x8d)</p> <code>ResNeXt101_64X4D</code> <p>ResNext-101 (32x4d)</p> <code>ResNeXt50_32X4D</code> <p>ResNext-50 (32x4d)</p> <code>ResNet</code> <p>A residual neural network for image classification.</p> <code>ResNet101</code> <p>ResNet-101</p> <code>ResNet18</code> <p>ResNet-18</p> <code>ResNet34</code> <p>ResNet-34</p> <code>ResNet50</code> <p>ResNet-50</p> <code>WideResNet101</code> <p>WideResNet-101-2</p> <code>WideResNet50</code> <p>WideResNet-50-2</p> <p>Attributes:</p> Name Type Description <code>___all__</code>"},{"location":"api/models/#inferno.models.___all__","title":"___all__","text":"<pre><code>___all__ = [\n    \"Ensemble\",\n    \"LeNet5\",\n    \"MLP\",\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNeXt50_32X4D\",\n    \"ResNeXt101_32X8D\",\n    \"ResNeXt101_64X4D\",\n    \"WideResNet50\",\n    \"WideResNet101\",\n    \"as_torch_model\",\n]\n</code></pre>"},{"location":"api/models/#inferno.models.Ensemble","title":"Ensemble","text":"<pre><code>Ensemble(members: Iterable[Module])\n</code></pre> <p>               Bases: <code>BNNModule</code></p> <p>An ensemble of models.</p> <p>This class ensembles multiple models with the same architecture by averaging their predictions.</p> <p>Parameters:</p> Name Type Description Default <code>members</code> <code>Iterable[Module]</code> <p>List of models to ensemble.</p> required <p>Methods:</p> Name Description <code>forward</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>base_module</code> <code>members</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.Ensemble.base_module","title":"base_module","text":"<pre><code>base_module = [to(device='meta')]\n</code></pre>"},{"location":"api/models/#inferno.models.Ensemble.members","title":"members","text":"<pre><code>members = ModuleList(members)\n</code></pre>"},{"location":"api/models/#inferno.models.Ensemble.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.Ensemble.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*batch in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.Ensemble.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.Ensemble.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.LeNet5","title":"LeNet5","text":"<pre><code>LeNet5(\n    out_size: int = 10,\n    parametrization: Parametrization = MaximalUpdate(),\n    cov: FactorizedCovariance | None = None,\n    activation_layer: Callable[..., Module] | None = ReLU,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>A simple convolutional neural network for image classification of 28x28 grayscale images.</p> <p>Parameters:</p> Name Type Description Default <code>out_size</code> <code>int</code> <p>Size of the output (i.e. number of classes).</p> <code>10</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>Covariance structure of the weights.</p> <code>None</code> <code>activation_layer</code> <code>Callable[..., Module] | None</code> <p>Activation function following a linear layer.</p> <code>ReLU</code> <p>Methods:</p> Name Description <code>forward</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>out_size</code> <code>parametrization</code> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.LeNet5.out_size","title":"out_size","text":"<pre><code>out_size = out_size\n</code></pre>"},{"location":"api/models/#inferno.models.LeNet5.parametrization","title":"parametrization","text":"<pre><code>parametrization = parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.LeNet5.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*batch in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.LeNet5.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.LeNet5.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.MLP","title":"MLP","text":"<pre><code>MLP(\n    in_size: int,\n    hidden_sizes: list[int],\n    out_size: int,\n    norm_layer: Callable[..., Module] | None = None,\n    activation_layer: Callable[..., Module] | None = ReLU,\n    inplace: bool | None = None,\n    bias: bool = True,\n    dropout: float = 0.0,\n    parametrization: Parametrization = MaximalUpdate(),\n    cov: (\n        FactorizedCovariance\n        | list[FactorizedCovariance]\n        | None\n    ) = None,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>A fully-connected feedforward neural network with the same activation function in each layer.</p> <p>Parameters:</p> Name Type Description Default <code>in_size</code> <code>int</code> <p>Size of the input.</p> required <code>hidden_sizes</code> <code>list[int]</code> <p>List of hidden layer sizes.</p> required <code>out_size</code> <code>int</code> <p>Size of the output (e.g. number of classes).</p> required <code>norm_layer</code> <code>Callable[..., Module] | None</code> <p>Normalization layer which will be stacked on top of the linear layer.</p> <code>None</code> <code>activation_layer</code> <code>Callable[..., Module] | None</code> <p>Activation function following a linear layer.</p> <code>ReLU</code> <code>inplace</code> <code>bool | None</code> <p>Whether to apply the activation function and dropout inplace. Default is <code>None</code>, which uses the respective default values.</p> <code>None</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in the linear layer.``</p> <code>True</code> <code>dropout</code> <code>float</code> <p>The probability for the dropout layer.</p> <code>0.0</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>cov</code> <code>FactorizedCovariance | list[FactorizedCovariance] | None</code> <p>Covariance structure of the weights.</p> <code>None</code> <p>Methods:</p> Name Description <code>forward</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>hidden_sizes</code> <code>in_size</code> <code>out_size</code> <code>parametrization</code> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.MLP.hidden_sizes","title":"hidden_sizes","text":"<pre><code>hidden_sizes = hidden_sizes\n</code></pre>"},{"location":"api/models/#inferno.models.MLP.in_size","title":"in_size","text":"<pre><code>in_size = in_size\n</code></pre>"},{"location":"api/models/#inferno.models.MLP.out_size","title":"out_size","text":"<pre><code>out_size = out_size\n</code></pre>"},{"location":"api/models/#inferno.models.MLP.parametrization","title":"parametrization","text":"<pre><code>parametrization = parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.MLP.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*batch in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.MLP.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.MLP.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D","title":"ResNeXt101_32X8D","text":"<pre><code>ResNeXt101_32X8D(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNext-101 (32x8d)</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to :class:<code>~inferno.bnn.models.ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D","title":"ResNeXt101_64X4D","text":"<pre><code>ResNeXt101_64X4D(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNext-101 (32x4d)</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to :class:<code>~inferno.bnn.models.ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D","title":"ResNeXt50_32X4D","text":"<pre><code>ResNeXt50_32X4D(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNext-50 (32x4d)</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to :class:<code>~inferno.bnn.models.ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNet","title":"ResNet","text":"<pre><code>ResNet(\n    out_size: int,\n    block: type[\"BasicBlock\"] | type[\"Bottleneck\"],\n    num_blocks_per_layer: Sequence[int],\n    zero_init_residual: bool = False,\n    groups: int = 1,\n    width_per_group: int = 64,\n    replace_stride_with_dilation: Sequence[bool] = (\n        False,\n        False,\n        False,\n    ),\n    norm_layer: Callable[..., Module] = lambda c: GroupNorm(\n        num_groups=32, num_channels=c\n    ),\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    parametrization: Parametrization = MaximalUpdate(),\n    cov: FactorizedCovariance | None = None,\n)\n</code></pre> <p>               Bases: <code>BNNModule</code></p> <p>A residual neural network for image classification.</p> <p>Parameters:</p> Name Type Description Default <code>out_size</code> <code>int</code> <p>Size of the output (i.e. number of classes).</p> required <code>block</code> <code>type['BasicBlock'] | type['Bottleneck']</code> <p>Block type to use.</p> required <code>num_blocks_per_layer</code> <code>Sequence[int]</code> <p>Number of blocks per layer.</p> required <code>zero_init_residual</code> <code>bool</code> <code>False</code> <code>groups</code> <code>int</code> <p>Number of groups for the convolutional layers.</p> <code>1</code> <code>width_per_group</code> <code>int</code> <p>Width per group for the convolutional layers.</p> <code>64</code> <code>replace_stride_with_dilation</code> <code>Sequence[bool]</code> <p>Whether to replace the 2x2 stride with a dilated convolution. Must be a tuple of length 3.</p> <code>(False, False, False)</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer to use.</p> <code>lambda c: GroupNorm(num_groups=32, num_channels=c)</code> <code>architecture</code> <code>Literal['imagenet', 'cifar']</code> <p>Type of ResNet architecture. Either \"imagenet\" or \"cifar\".</p> <code>'imagenet'</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>Covariance structure of the probabilistic layers.</p> <code>None</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <p>Load a ResNet model with pretrained weights.</p> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNet.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNet.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    weights: Weights,\n    freeze: bool = False,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    *args,\n    **kwargs\n)\n</code></pre> <p>Load a ResNet model with pretrained weights.</p> <p>Depending on the <code>out_size</code> and <code>architecture</code> parameters, the first and last layers of the model are not initialized with the pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>out_size</code> <code>int</code> <p>Size of the output (i.e. number of classes).</p> required <code>weights</code> <code>Weights</code> <p>Pretrained weights to use.</p> required <code>freeze</code> <code>bool</code> <p>Whether to freeze the pretrained weights.</p> <code>False</code> <code>architecture</code> <code>Literal['imagenet', 'cifar']</code> <p>Type of ResNet architecture. Either \"imagenet\" or \"cifar\".</p> <code>'imagenet'</code>"},{"location":"api/models/#inferno.models.ResNet.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNet.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNet101","title":"ResNet101","text":"<pre><code>ResNet101(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNet-101</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to :class:<code>~inferno.bnn.models.ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNet101.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNet101.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNet101.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNet18","title":"ResNet18","text":"<pre><code>ResNet18(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNet-18</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to :class:<code>~inferno.bnn.models.ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNet18.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNet18.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNet18.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNet34","title":"ResNet34","text":"<pre><code>ResNet34(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNet-34</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to :class:<code>~inferno.bnn.models.ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNet34.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNet34.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNet34.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNet50","title":"ResNet50","text":"<pre><code>ResNet50(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNet-50</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to :class:<code>~inferno.bnn.models.ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNet50.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNet50.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNet50.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.WideResNet101","title":"WideResNet101","text":"<pre><code>WideResNet101(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>WideResNet-101-2</p> <p>Architecture described in Wide Residual Networks. The model is the same as a ResNet except for the bottleneck number of channels which is twice larger in every block.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to :class:<code>~inferno.bnn.models.ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.WideResNet101.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.WideResNet101.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.WideResNet101.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.WideResNet50","title":"WideResNet50","text":"<pre><code>WideResNet50(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>WideResNet-50-2</p> <p>Architecture described in Wide Residual Networks. The model is the same as a ResNet except for the bottleneck number of channels which is twice larger in every block.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to :class:<code>~inferno.bnn.models.ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.WideResNet50.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else deepcopy(cov)\n    ),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.WideResNet50.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.WideResNet50.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/bnn/modules/","title":"Modules","text":""},{"location":"api/bnn/modules/#inferno.bnn.modules","title":"modules","text":"<p>Classes:</p> Name Description <code>BNNModule</code> <p>Base class for all Bayesian neural network modules.</p> <code>Conv1d</code> <p>Applies a 1D convolution over an input signal composed of several input planes.</p> <code>Conv2d</code> <p>Applies a 2D convolution over an input signal composed of several input planes.</p> <code>Conv3d</code> <p>Applies a 3D convolution over an input signal composed of several input planes.</p> <code>Linear</code> <p>Applies an affine transformation to the input.</p> <code>Sequential</code> <p>A sequential container for modules.</p> <p>Functions:</p> Name Description <code>batched_forward</code> <p>Call a torch.nn.Module on inputs with arbitrary many batch dimensions rather than</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.BNNModule","title":"BNNModule","text":"<pre><code>BNNModule(\n    parametrization: (\n        Parametrization | None\n    ) = MaximalUpdate(),\n)\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Base class for all Bayesian neural network modules.</p> <p>Parameters:</p> Name Type Description Default <code>parametrization</code> <code>Parametrization | None</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass of the module.</p> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.BNNModule.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.BNNModule.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre> <p>Forward pass of the module.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Float[Tensor, '*sample batch *in_feature']</code> <p>Input tensor.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of samples.</p> <code>Size([])</code> <code>generator</code> <code>Generator | None</code> <p>Random number generator.</p> <code>None</code> <code>input_contains_samples</code> <code>bool</code> <p>Whether the input already contains samples. If True, the input is assumed to have <code>len(sample_shape)</code> many leading dimensions containing input samples (typically outputs from previous layers).</p> <code>False</code> <code>parameter_samples</code> <code>dict[str, Float[Tensor, '*sample parameter']] | None</code> <p>Dictionary of parameter samples. Used to pass sampled parameters to the module. Useful to jointly sample parameters of multiple layers.</p> <code>None</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.BNNModule.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/bnn/modules/#inferno.bnn.modules.BNNModule.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d","title":"Conv1d","text":"<pre><code>Conv1d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_1_t,\n    stride: _size_1_t = 1,\n    padding: str | _size_1_t = 0,\n    dilation: _size_1_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"zeros\",\n    layer_type: Literal[\n        \"input\", \"hidden\", \"output\"\n    ] = \"hidden\",\n    cov: FactorizedCovariance | None = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    device: device = None,\n    dtype: dtype = None,\n)\n</code></pre> <p>               Bases: <code>_ConvNd</code></p> <p>Applies a 1D convolution over an input signal composed of several input planes.</p> <p>In the simplest case, the output value of the layer with input size :math:<code>(N, C_{\\text{in}}, L)</code> and output :math:<code>(N, C_{\\text{out}}, L_{\\text{out}})</code> can be precisely described as:</p> <p>.. math::     \\text{out}(N_i, C_{\\text{out}j}) = \\text{bias}(Cj}) +     \\sum, k)     \\star \\text{input}(N_i, k)}^{C_{in} - 1} \\text{weight}(C_{\\text{out}_j</p> <p>where :math:<code>\\star</code> is the valid <code>cross-correlation</code>_ operator, :math:<code>N</code> is a batch size, :math:<code>C</code> denotes a number of channels, :math:<code>L</code> is a length of signal sequence.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>_size_1_t</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>_size_1_t</code> <p>Stride of the convolution.</p> <code>1</code> <code>padding</code> <code>str | _size_1_t</code> <p>Padding added to both sides of the input.</p> <code>0</code> <code>dilation</code> <code>_size_1_t</code> <p>Spacing between kernel elements.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default is <code>'zeros'</code>.</p> <code>'zeros'</code> <code>layer_type</code> <code>Literal['input', 'hidden', 'output']</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\". Controls the initialization and learning rate scaling of the parameters.</p> <code>'hidden'</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>The covariance of the parameters.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>device</code> <code>device</code> <p>The device on which to place the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The desired data type of the returned tensor.</p> <code>None</code> <p>Methods:</p> Name Description <code>extra_repr</code> <code>forward</code> <code>parameters_and_lrs</code> <code>reset_parameters</code> <p>Reset the parameters of the module.</p> <p>Attributes:</p> Name Type Description <code>bias</code> <code>Parameter</code> <code>dilation</code> <code>groups</code> <code>in_channels</code> <code>kernel_size</code> <code>layer_type</code> <code>out_channels</code> <code>output_padding</code> <code>padding</code> <code>padding_mode</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>params</code> <code>stride</code> <code>transposed</code> <code>weight</code> <code>Parameter</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.bias","title":"bias","text":"<pre><code>bias: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.dilation","title":"dilation","text":"<pre><code>dilation = dilation\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.in_channels","title":"in_channels","text":"<pre><code>in_channels = in_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.kernel_size","title":"kernel_size","text":"<pre><code>kernel_size = kernel_size\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.layer_type","title":"layer_type","text":"<pre><code>layer_type = layer_type\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.out_channels","title":"out_channels","text":"<pre><code>out_channels = out_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.output_padding","title":"output_padding","text":"<pre><code>output_padding = output_padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.padding","title":"padding","text":"<pre><code>padding = padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.padding_mode","title":"padding_mode","text":"<pre><code>padding_mode = padding_mode\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.params","title":"params","text":"<pre><code>params = ParameterDict(mean_param_dict)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.stride","title":"stride","text":"<pre><code>stride = stride\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.transposed","title":"transposed","text":"<pre><code>transposed = transposed\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.weight","title":"weight","text":"<pre><code>weight: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[\n    Tensor, \"*sample *batch out_channel *out_feature\"\n]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"] = \"SGD\"\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d","title":"Conv2d","text":"<pre><code>Conv2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: str | _size_2_t = 0,\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"zeros\",\n    layer_type: Literal[\n        \"input\", \"hidden\", \"output\"\n    ] = \"hidden\",\n    cov: FactorizedCovariance | None = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    device: device = None,\n    dtype: dtype = None,\n)\n</code></pre> <p>               Bases: <code>_ConvNd</code></p> <p>Applies a 2D convolution over an input signal composed of several input planes.</p> <p>In the simplest case, the output value of the layer with input size :math:<code>(N, C_{\\text{in}}, H, W)</code> and output :math:<code>(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})</code> can be precisely described as:</p> <p>.. math::     \\text{out}(N_i, C_{\\text{out}j}) = \\text{bias}(Cj}) +     \\sum(N_i, k)}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input</p> <p>where :math:<code>\\star</code> is the valid 2D <code>cross-correlation</code>_ operator, :math:<code>N</code> is a batch size, :math:<code>C</code> denotes a number of channels, :math:<code>H</code> is a height of input planes in pixels, and :math:<code>W</code> is width in pixels.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the convolution.</p> <code>1</code> <code>padding</code> <code>str | _size_2_t</code> <p>Padding added to all four sides of the input.</p> <code>0</code> <code>dilation</code> <code>_size_2_t</code> <p>Spacing between kernel elements.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default is <code>'zeros'</code>.</p> <code>'zeros'</code> <code>layer_type</code> <code>Literal['input', 'hidden', 'output']</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\". Controls the initialization and learning rate scaling of the parameters.</p> <code>'hidden'</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>The covariance of the parameters.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>device</code> <code>device</code> <p>The device on which to place the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The desired data type of the returned tensor.</p> <code>None</code> <p>Methods:</p> Name Description <code>extra_repr</code> <code>forward</code> <code>parameters_and_lrs</code> <code>reset_parameters</code> <p>Reset the parameters of the module.</p> <p>Attributes:</p> Name Type Description <code>bias</code> <code>Parameter</code> <code>dilation</code> <code>groups</code> <code>in_channels</code> <code>kernel_size</code> <code>layer_type</code> <code>out_channels</code> <code>output_padding</code> <code>padding</code> <code>padding_mode</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>params</code> <code>stride</code> <code>transposed</code> <code>weight</code> <code>Parameter</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.bias","title":"bias","text":"<pre><code>bias: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.dilation","title":"dilation","text":"<pre><code>dilation = dilation\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.in_channels","title":"in_channels","text":"<pre><code>in_channels = in_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.kernel_size","title":"kernel_size","text":"<pre><code>kernel_size = kernel_size\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.layer_type","title":"layer_type","text":"<pre><code>layer_type = layer_type\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.out_channels","title":"out_channels","text":"<pre><code>out_channels = out_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.output_padding","title":"output_padding","text":"<pre><code>output_padding = output_padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.padding","title":"padding","text":"<pre><code>padding = padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.padding_mode","title":"padding_mode","text":"<pre><code>padding_mode = padding_mode\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.params","title":"params","text":"<pre><code>params = ParameterDict(mean_param_dict)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.stride","title":"stride","text":"<pre><code>stride = stride\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.transposed","title":"transposed","text":"<pre><code>transposed = transposed\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.weight","title":"weight","text":"<pre><code>weight: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[\n    Tensor, \"*sample *batch out_channel *out_feature\"\n]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"] = \"SGD\"\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d","title":"Conv3d","text":"<pre><code>Conv3d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_3_t,\n    stride: _size_3_t = 1,\n    padding: str | _size_3_t = 0,\n    dilation: _size_3_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"zeros\",\n    layer_type: Literal[\n        \"input\", \"hidden\", \"output\"\n    ] = \"hidden\",\n    cov: FactorizedCovariance | None = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    device: device = None,\n    dtype: dtype = None,\n)\n</code></pre> <p>               Bases: <code>_ConvNd</code></p> <p>Applies a 3D convolution over an input signal composed of several input planes.</p> <p>In the simplest case, the output value of the layer with input size :math:<code>(N, C_{in}, D, H, W)</code> and output :math:<code>(N, C_{out}, D_{out}, H_{out}, W_{out})</code> can be precisely described as:</p> <p>.. math::     out(N_i, C_{out_j}) = bias(C_{out_j}) +                             \\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\star input(N_i, k)</p> <p>where :math:<code>\\star</code> is the valid 3D <code>cross-correlation</code>_ operator</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>_size_3_t</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>_size_3_t</code> <p>Stride of the convolution.</p> <code>1</code> <code>padding</code> <code>str | _size_3_t</code> <p>Padding added to all six sides of the input.</p> <code>0</code> <code>dilation</code> <code>_size_3_t</code> <p>Spacing between kernel elements.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default is <code>'zeros'</code>.</p> <code>'zeros'</code> <code>layer_type</code> <code>Literal['input', 'hidden', 'output']</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\". Controls the initialization and learning rate scaling of the parameters.</p> <code>'hidden'</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>The covariance of the parameters.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>device</code> <code>device</code> <p>The device on which to place the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The desired data type of the returned tensor.</p> <code>None</code> <p>Methods:</p> Name Description <code>extra_repr</code> <code>forward</code> <code>parameters_and_lrs</code> <code>reset_parameters</code> <p>Reset the parameters of the module.</p> <p>Attributes:</p> Name Type Description <code>bias</code> <code>Parameter</code> <code>dilation</code> <code>groups</code> <code>in_channels</code> <code>kernel_size</code> <code>layer_type</code> <code>out_channels</code> <code>output_padding</code> <code>padding</code> <code>padding_mode</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>params</code> <code>stride</code> <code>transposed</code> <code>weight</code> <code>Parameter</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.bias","title":"bias","text":"<pre><code>bias: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.dilation","title":"dilation","text":"<pre><code>dilation = dilation\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.in_channels","title":"in_channels","text":"<pre><code>in_channels = in_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.kernel_size","title":"kernel_size","text":"<pre><code>kernel_size = kernel_size\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.layer_type","title":"layer_type","text":"<pre><code>layer_type = layer_type\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.out_channels","title":"out_channels","text":"<pre><code>out_channels = out_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.output_padding","title":"output_padding","text":"<pre><code>output_padding = output_padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.padding","title":"padding","text":"<pre><code>padding = padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.padding_mode","title":"padding_mode","text":"<pre><code>padding_mode = padding_mode\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.params","title":"params","text":"<pre><code>params = ParameterDict(mean_param_dict)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.stride","title":"stride","text":"<pre><code>stride = stride\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.transposed","title":"transposed","text":"<pre><code>transposed = transposed\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.weight","title":"weight","text":"<pre><code>weight: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[\n    Tensor, \"*sample *batch out_channel *out_feature\"\n]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"] = \"SGD\"\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear","title":"Linear","text":"<pre><code>Linear(\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    layer_type: Literal[\n        \"input\", \"hidden\", \"output\"\n    ] = \"hidden\",\n    cov: FactorizedCovariance | None = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    device: device | None = None,\n    dtype: dtype | None = None,\n)\n</code></pre> <p>               Bases: <code>BNNModule</code></p> <p>Applies an affine transformation to the input.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Size of each input sample.</p> required <code>out_features</code> <code>int</code> <p>Size of each output sample.</p> required <code>bias</code> <code>bool</code> <p>If set to <code>False</code>, the layer will not learn an additive bias.</p> <code>True</code> <code>layer_type</code> <code>Literal['input', 'hidden', 'output']</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\". Controls the initialization and learning rate scaling of the parameters.</p> <code>'hidden'</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>Covariance object for the parameters.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>device</code> <code>device | None</code> <p>Device on which to create the parameters.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Data type of the parameters.</p> <code>None</code> <p>Methods:</p> Name Description <code>extra_repr</code> <code>forward</code> <code>parameters_and_lrs</code> <code>reset_parameters</code> <p>Reset the parameters of the module.</p> <p>Attributes:</p> Name Type Description <code>bias</code> <code>Parameter</code> <code>in_features</code> <code>layer_type</code> <code>out_features</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>params</code> <code>weight</code> <code>Parameter</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.bias","title":"bias","text":"<pre><code>bias: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.in_features","title":"in_features","text":"<pre><code>in_features = in_features\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.layer_type","title":"layer_type","text":"<pre><code>layer_type = layer_type\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.out_features","title":"out_features","text":"<pre><code>out_features = out_features\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.params","title":"params","text":"<pre><code>params = ParameterDict(mean_param_dict)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.weight","title":"weight","text":"<pre><code>weight: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample *batch in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch out_feature\"]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"] = \"SGD\"\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Sequential","title":"Sequential","text":"<pre><code>Sequential(\n    *args: BNNModule,\n    parametrization: Parametrization | None = None\n)\n</code></pre><pre><code>Sequential(\n    arg: OrderedDict[str, BNNModule],\n    parametrization: Parametrization | None = None,\n)\n</code></pre> <pre><code>Sequential(\n    *args, parametrization: Parametrization | None = None\n)\n</code></pre> <p>               Bases: <code>BNNModule</code>, <code>Sequential</code></p> <p>A sequential container for modules.</p> <p>Modules will be added to it in the order they are passed in the constructor. Alternatively, an <code>OrderedDict</code> of modules can be passed in. The <code>forward()</code> method of <code>Sequential</code> accepts any input and forwards it to the first module it contains. It then \"chains\" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.</p> <p>The value a <code>Sequential</code> provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the <code>Sequential</code> applies to each of the modules it stores (which are each a registered submodule of the <code>Sequential</code>)</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Any number of modules to add to the container.</p> <code>()</code> <code>parametrization</code> <code>Parametrization | None</code> <p>The parametrization to use. If <code>None</code>, the parametrization of the modules in the container will be used. If a :class:<code>~inferno.bnn.params.Parametrization</code> object is passed, it will be used for all modules in the container.</p> <code>None</code> <p>Methods:</p> Name Description <code>forward</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>parametrization</code> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Sequential.parametrization","title":"parametrization","text":"<pre><code>parametrization = parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Sequential.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*batch in_feature\"],\n    /,\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch out_feature\"]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Sequential.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/bnn/modules/#inferno.bnn.modules.Sequential.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.batched_forward","title":"batched_forward","text":"<pre><code>batched_forward(\n    obj: Module, num_batch_dims: int\n) -&gt; Callable[\n    [Float[Tensor, \"*sample batch *in_feature\"]],\n    Float[Tensor, \"*sample batch *out_feature\"],\n]\n</code></pre> <p>Call a torch.nn.Module on inputs with arbitrary many batch dimensions rather than just a single one.</p> <p>This is useful to extend the functionality of a torch.nn.Module to work with arbitrary many batch dimensions, for example arbitrary many sampling dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Module</code> <p>The torch.nn.Module to call.</p> required <code>num_batch_dims</code> <code>int</code> <p>The number of batch dimensions.</p> required"},{"location":"api/bnn/temperature_scaling/","title":"Temperature Scaling","text":""},{"location":"api/bnn/temperature_scaling/#inferno.bnn","title":"bnn","text":"<p>Basic building blocks for Bayesian neural networks.</p> <p>Classes:</p> Name Description <code>TemperatureScaler</code> <p>Temperature scaling.</p>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler","title":"TemperatureScaler","text":"<pre><code>TemperatureScaler(\n    loss_fn: Module = CrossEntropyLoss(),\n    lr: float = 0.1,\n    max_iter: int = 100,\n    tolerance_grad: float = 1e-07,\n    tolerance_change: float = 1e-09,\n    history_size: int = 100,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Temperature scaling.</p> <p>Tunes the temperature parameter of the model in the last layer to minimize the negative log likelihood of the validation set.</p> <p>Based on On Calibration of Modern Neural Networks.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Module</code> <p>The loss function to be used for calibration.</p> <code>CrossEntropyLoss()</code> <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.1</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations per optimization step.</p> <code>100</code> <code>tolerance_grad</code> <code>float</code> <p>Tolerance for the gradient.</p> <code>1e-07</code> <code>tolerance_change</code> <code>float</code> <p>Tolerance for the change in the loss function / parameters.</p> <code>1e-09</code> <code>history_size</code> <code>int</code> <p>Size of the history for the LBFGS optimizer.</p> <code>100</code> <p>Methods:</p> Name Description <code>optimize</code> <p>Optimizes the temperature of the model.</p> <p>Attributes:</p> Name Type Description <code>history_size</code> <code>loss_fn</code> <code>lr</code> <code>max_iter</code> <code>tolerance_change</code> <code>tolerance_grad</code>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.history_size","title":"history_size","text":"<pre><code>history_size = history_size\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.loss_fn","title":"loss_fn","text":"<pre><code>loss_fn = loss_fn\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.lr","title":"lr","text":"<pre><code>lr = lr\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.max_iter","title":"max_iter","text":"<pre><code>max_iter = max_iter\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.tolerance_change","title":"tolerance_change","text":"<pre><code>tolerance_change = tolerance_change\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.tolerance_grad","title":"tolerance_grad","text":"<pre><code>tolerance_grad = tolerance_grad\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.optimize","title":"optimize","text":"<pre><code>optimize(model: BNNModule, dataloader: DataLoader) -&gt; None\n</code></pre> <p>Optimizes the temperature of the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BNNModule</code> <p>The model to be calibrated, assumed to return logits.</p> required <code>dataloader</code> <code>DataLoader</code> <p>The dataloader for the dataset to calibrate on (typically the validation set).</p> required"},{"location":"api/bnn/bnn.params/parameters/","title":"Parameters","text":""},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params","title":"params","text":"<p>Classes:</p> Name Description <code>BNNParameter</code> <code>GaussianParameter</code> <p>Parameter of a BNNModule with Gaussian distribution.</p> <code>FactorizedCovariance</code> <p>Covariance of a Gaussian parameter with a factorized structure.</p> <code>DiagonalCovariance</code> <p>Covariance of a Gaussian parameter with diagonal structure.</p> <code>KroneckerCovariance</code> <p>Covariance of a Gaussian parameter with Kronecker structure.</p> <code>LowRankCovariance</code> <p>Covariance of a Gaussian parameter with low-rank structure.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.BNNParameter","title":"BNNParameter","text":"<pre><code>BNNParameter(\n    hyperparameters: (\n        dict[str, Float[Tensor, \"*hyperparameter\"]] | None\n    ) = None,\n)\n</code></pre> <p>               Bases: <code>ParameterDict</code>, <code>ABC</code></p> <p>Methods:</p> Name Description <code>sample</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.BNNParameter.sample","title":"sample","text":"<pre><code>sample(\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n) -&gt; Float[Tensor, \"*sample parameter\"]\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.GaussianParameter","title":"GaussianParameter","text":"<pre><code>GaussianParameter(\n    mean: (\n        Float[Tensor, \"parameter\"]\n        | dict[str, Float[Tensor, \"parameter\"]]\n    ),\n    cov: FactorizedCovariance,\n)\n</code></pre> <p>               Bases: <code>BNNParameter</code></p> <p>Parameter of a BNNModule with Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Float[Tensor, 'parameter'] | dict[str, Float[Tensor, 'parameter']]</code> <p>Mean of the Gaussian distribution.</p> required <code>cov</code> <code>FactorizedCovariance</code> <p>Covariance of the Gaussian distribution.</p> required <p>Methods:</p> Name Description <code>sample</code> <p>Attributes:</p> Name Type Description <code>cov</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.GaussianParameter.cov","title":"cov","text":"<pre><code>cov = cov\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.GaussianParameter.sample","title":"sample","text":"<pre><code>sample(\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n) -&gt; (\n    Float[Tensor, \"*sample parameter\"]\n    | dict[str, Float[Tensor, \"*sample parameter\"]]\n)\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance","title":"FactorizedCovariance","text":"<pre><code>FactorizedCovariance(rank: int | None = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Covariance of a Gaussian parameter with a factorized structure.</p> <p>Assumes the covariance is factorized as a product of a square matrix and its transpose.</p> <p>..math::     \\mathbf{\\Sigma} = \\mathbf{S} \\mathbf{S}^\\top</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int | None</code> <p>Rank of the covariance matrix. If <code>None</code>, the rank is set to the total number of mean parameters.</p> <code>None</code> <p>Methods:</p> Name Description <code>factor_matmul</code> <p>Multiply left factor of the covariance matrix with the input.</p> <code>initialize_parameters</code> <p>Initialize the covariance parameters.</p> <code>reset_parameters</code> <p>Reset the parameters of the covariance matrix.</p> <code>to_dense</code> <p>Convert the covariance matrix to a dense representation.</p> <p>Attributes:</p> Name Type Description <code>lr_scaling</code> <code>dict[str, float]</code> <p>Compute the learning rate scaling for the covariance parameters.</p> <code>rank</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.lr_scaling","title":"lr_scaling","text":"<pre><code>lr_scaling: dict[str, float]\n</code></pre> <p>Compute the learning rate scaling for the covariance parameters.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.rank","title":"rank","text":"<pre><code>rank = rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.factor_matmul","title":"factor_matmul","text":"<pre><code>factor_matmul(\n    input: Float[Tensor, \"*sample parameter\"],\n    /,\n    additive_constant: (\n        Float[Tensor, \"*sample parameter\"] | None\n    ) = None,\n) -&gt; dict[str, Float[Tensor, \"*sample parameter\"]]\n</code></pre> <p>Multiply left factor of the covariance matrix with the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Float[Tensor, '*sample parameter']</code> <p>Input tensor.</p> required <code>additive_constant</code> <code>Float[Tensor, '*sample parameter'] | None</code> <p>Additive constant to be added to the output.</p> <code>None</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.initialize_parameters","title":"initialize_parameters","text":"<pre><code>initialize_parameters(\n    mean_parameters: dict[str, Tensor],\n) -&gt; None\n</code></pre> <p>Initialize the covariance parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mean_parameters</code> <code>dict[str, Tensor]</code> <p>Mean parameters of the Gaussian distribution.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Covariance parameters.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters(\n    mean_parameter_scales: dict[str, float] | float = 1.0,\n) -&gt; None\n</code></pre> <p>Reset the parameters of the covariance matrix.</p> <p>Initalizes the parameters of the covariance matrix with a scale that is given by the mean parameter scales and a covariance-specific scaling that depends on the structure of the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>mean_parameter_scales</code> <code>dict[str, float] | float</code> <p>Scales of the mean parameters. If a dictionary keys are the names of the mean parameters. If a float, all covariance parameters are initialized with the same scale.</p> <code>1.0</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.to_dense","title":"to_dense","text":"<pre><code>to_dense() -&gt; Float[Tensor, 'parameter parameter']\n</code></pre> <p>Convert the covariance matrix to a dense representation.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance","title":"DiagonalCovariance","text":"<pre><code>DiagonalCovariance()\n</code></pre> <p>               Bases: <code>FactorizedCovariance</code></p> <p>Covariance of a Gaussian parameter with diagonal structure.</p> <p>Methods:</p> Name Description <code>factor_matmul</code> <code>initialize_parameters</code> <code>reset_parameters</code> <code>to_dense</code> <p>Convert the covariance matrix to a dense representation.</p> <p>Attributes:</p> Name Type Description <code>lr_scaling</code> <code>dict[str, float]</code> <code>rank</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.lr_scaling","title":"lr_scaling","text":"<pre><code>lr_scaling: dict[str, float]\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.rank","title":"rank","text":"<pre><code>rank = rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.factor_matmul","title":"factor_matmul","text":"<pre><code>factor_matmul(\n    input: Float[Tensor, \"*sample parameter\"],\n    /,\n    additive_constant: (\n        Float[Tensor, \"*sample parameter\"] | None\n    ) = None,\n) -&gt; dict[str, Float[Tensor, \"*sample parameter\"]]\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.initialize_parameters","title":"initialize_parameters","text":"<pre><code>initialize_parameters(\n    mean_parameters: dict[str, Tensor],\n) -&gt; None\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters(\n    mean_parameter_scales: dict[str, float] | float = 1.0,\n) -&gt; None\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.to_dense","title":"to_dense","text":"<pre><code>to_dense() -&gt; Float[Tensor, 'parameter parameter']\n</code></pre> <p>Convert the covariance matrix to a dense representation.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance","title":"KroneckerCovariance","text":"<pre><code>KroneckerCovariance(\n    input_rank: int | None = None,\n    output_rank: int | None = None,\n)\n</code></pre> <p>               Bases: <code>FactorizedCovariance</code></p> <p>Covariance of a Gaussian parameter with Kronecker structure.</p> <p>Assumes the covariance is given by a Kronecker product of two matrices of size equal to the number of inputs and outputs to the layer. Each Kronecker factor is assumed to be of rank \\(R \\leq D\\) where \\(D\\) is either the input or output dimension of the layer.</p> <p>More precisely, the covariance is given by</p> \\[ \\begin{align*}     \\mathbf{\\Sigma} &amp;= \\mathbf{C}_{\\text{in}} \\otimes \\mathbf{C}_{\\text{out}}\\\\                     &amp;= \\mathbf{S}_{\\text{in}}\\mathbf{S}_{\\text{in}}^\\top \\otimes \\mathbf{S}_{\\text{out}}\\mathbf{S}_{\\text{out}}^\\top\\\\                     &amp;= (\\mathbf{S}_{\\text{in}} \\otimes \\mathbf{S}_{\\text{out}}) (\\mathbf{S}_{\\text{in}}^\\top \\otimes \\mathbf{S}_{\\text{out}}^\\top) \\end{align*} \\] <p>where \\(\\mathbf{S}_{\\text{in}}\\) and \\(\\mathbf{S}_{\\text{out}}\\) are the low-rank factors of the Kronecker factors \\(\\mathbf{C}_{\\text{in}}\\) and  \\(\\mathbf{C}_{\\text{out}}\\).</p> <p>Parameters:</p> Name Type Description Default <code>input_rank</code> <code>int | None</code> <p>Rank of the input Kronecker factor. If None, assumes full rank.</p> <code>None</code> <code>output_rank</code> <code>int | None</code> <p>Rank of the output Kronecker factor. If None, assumes full rank.</p> <code>None</code> <p>Methods:</p> Name Description <code>factor_matmul</code> <code>initialize_parameters</code> <code>reset_parameters</code> <code>to_dense</code> <p>Attributes:</p> Name Type Description <code>input_rank</code> <code>lr_scaling</code> <code>dict[str, float]</code> <p>Compute the learning rate scaling for the covariance parameters.</p> <code>output_rank</code> <code>rank</code> <code>sample_scale</code> <code>float</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.input_rank","title":"input_rank","text":"<pre><code>input_rank = input_rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.lr_scaling","title":"lr_scaling","text":"<pre><code>lr_scaling: dict[str, float]\n</code></pre> <p>Compute the learning rate scaling for the covariance parameters.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.output_rank","title":"output_rank","text":"<pre><code>output_rank = output_rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.rank","title":"rank","text":"<pre><code>rank = rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.sample_scale","title":"sample_scale","text":"<pre><code>sample_scale: float\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.factor_matmul","title":"factor_matmul","text":"<pre><code>factor_matmul(\n    input: Float[Tensor, \"*sample parameter\"],\n    /,\n    additive_constant: (\n        Float[Tensor, \"*sample parameter\"] | None\n    ) = None,\n) -&gt; dict[str, Float[Tensor, \"*sample parameter\"]]\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.initialize_parameters","title":"initialize_parameters","text":"<pre><code>initialize_parameters(\n    mean_parameters: dict[str, Tensor],\n) -&gt; None\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters(\n    mean_parameter_scales: dict[str, float] | float = 1.0,\n) -&gt; None\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.to_dense","title":"to_dense","text":"<pre><code>to_dense() -&gt; Float[Tensor, 'parameter parameter']\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance","title":"LowRankCovariance","text":"<pre><code>LowRankCovariance(rank: int)\n</code></pre> <p>               Bases: <code>FactorizedCovariance</code></p> <p>Covariance of a Gaussian parameter with low-rank structure.</p> <p>Assumes the covariance is factorized as a product of a matrix :math:\\mathbf{S} \\in \\mathbb{R}^{P \\times R}` and its transpose.</p> <p>..math::     \\mathbf{\\Sigma} = \\mathbf{S} \\mathbf{S}^\\top</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Rank of the covariance matrix. If <code>None</code>, the rank is set to the total number of mean parameters.</p> required <p>Methods:</p> Name Description <code>factor_matmul</code> <p>Multiply left factor of the covariance matrix with the input.</p> <code>initialize_parameters</code> <p>Initialize the covariance parameters.</p> <code>reset_parameters</code> <p>Reset the parameters of the covariance matrix.</p> <code>to_dense</code> <p>Convert the covariance matrix to a dense representation.</p> <p>Attributes:</p> Name Type Description <code>lr_scaling</code> <code>dict[str, float]</code> <p>Compute the learning rate scaling for the covariance parameters.</p> <code>rank</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.lr_scaling","title":"lr_scaling","text":"<pre><code>lr_scaling: dict[str, float]\n</code></pre> <p>Compute the learning rate scaling for the covariance parameters.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.rank","title":"rank","text":"<pre><code>rank = rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.factor_matmul","title":"factor_matmul","text":"<pre><code>factor_matmul(\n    input: Float[Tensor, \"*sample parameter\"],\n    /,\n    additive_constant: (\n        Float[Tensor, \"*sample parameter\"] | None\n    ) = None,\n) -&gt; dict[str, Float[Tensor, \"*sample parameter\"]]\n</code></pre> <p>Multiply left factor of the covariance matrix with the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Float[Tensor, '*sample parameter']</code> <p>Input tensor.</p> required <code>additive_constant</code> <code>Float[Tensor, '*sample parameter'] | None</code> <p>Additive constant to be added to the output.</p> <code>None</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.initialize_parameters","title":"initialize_parameters","text":"<pre><code>initialize_parameters(\n    mean_parameters: dict[str, Tensor],\n) -&gt; None\n</code></pre> <p>Initialize the covariance parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mean_parameters</code> <code>dict[str, Tensor]</code> <p>Mean parameters of the Gaussian distribution.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Covariance parameters.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters(\n    mean_parameter_scales: dict[str, float] | float = 1.0,\n) -&gt; None\n</code></pre> <p>Reset the parameters of the covariance matrix.</p> <p>Initalizes the parameters of the covariance matrix with a scale that is given by the mean parameter scales and a covariance-specific scaling that depends on the structure of the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>mean_parameter_scales</code> <code>dict[str, float] | float</code> <p>Scales of the mean parameters. If a dictionary keys are the names of the mean parameters. If a float, all covariance parameters are initialized with the same scale.</p> <code>1.0</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.to_dense","title":"to_dense","text":"<pre><code>to_dense() -&gt; Float[Tensor, 'parameter parameter']\n</code></pre> <p>Convert the covariance matrix to a dense representation.</p>"},{"location":"api/bnn/bnn.params/parametrizations/","title":"Parametrizations","text":""},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations","title":"parametrizations","text":"<p>Classes:</p> Name Description <code>Parametrization</code> <p>Abstract base class for all neural network parametrizations.</p> <code>Standard</code> <p>Standard Parametrization (SP).</p> <code>NeuralTangent</code> <p>Neural Tangent Parametrization (NTP).</p> <code>MaximalUpdate</code> <p>Maximal update parametrization (\\(\\mu P\\)).</p>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Parametrization","title":"Parametrization","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all neural network parametrizations.</p> <p>Methods:</p> Name Description <code>bias_init_scale</code> <p>Compute the bias initialization scale for the given layer.</p> <code>bias_lr_scale</code> <p>Compute the learning rate scale for the bias parameters.</p> <code>weight_init_scale</code> <p>Compute the weight initialization scale for the given layer.</p> <code>weight_lr_scale</code> <p>Compute the learning rate scale for the weight parameters.</p>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Parametrization.bias_init_scale","title":"bias_init_scale","text":"<pre><code>bias_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre> <p>Compute the bias initialization scale for the given layer.</p> <p>Parameters:</p> Name Type Description Default <code>fan_in</code> <code>int</code> <p>Number of inputs to the layer.</p> required <code>fan_out</code> <code>int</code> <p>Number of outputs from the layer.</p> required <code>layer_type</code> <code>LayerType</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\".</p> <code>'hidden'</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Parametrization.bias_lr_scale","title":"bias_lr_scale","text":"<pre><code>bias_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre> <p>Compute the learning rate scale for the bias parameters.</p> <p>Parameters:</p> Name Type Description Default <code>fan_in</code> <code>int</code> <p>Number of inputs to the layer.</p> required <code>fan_out</code> <code>int</code> <p>Number of outputs from the layer.</p> required <code>optimizer</code> <code>str</code> <p>Optimizer being used.</p> required <code>layer_type</code> <code>LayerType</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\".</p> <code>'hidden'</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Parametrization.weight_init_scale","title":"weight_init_scale","text":"<pre><code>weight_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre> <p>Compute the weight initialization scale for the given layer.</p> <p>Parameters:</p> Name Type Description Default <code>fan_in</code> <code>int</code> <p>Number of inputs to the layer.</p> required <code>fan_out</code> <code>int</code> <p>Number of outputs from the layer.</p> required <code>layer_type</code> <code>LayerType</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\".</p> <code>'hidden'</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Parametrization.weight_lr_scale","title":"weight_lr_scale","text":"<pre><code>weight_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre> <p>Compute the learning rate scale for the weight parameters.</p> <p>Parameters:</p> Name Type Description Default <code>fan_in</code> <code>int</code> <p>Number of inputs to the layer.</p> required <code>fan_out</code> <code>int</code> <p>Number of outputs from the layer.</p> required <code>optimizer</code> <code>str</code> <p>Optimizer being used.</p> required <code>layer_type</code> <code>LayerType</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\".</p> <code>'hidden'</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Standard","title":"Standard","text":"<p>               Bases: <code>Parametrization</code></p> <p>Standard Parametrization (SP).</p> <p>The default parametrization for neural networks in PyTorch. Also known as the 'fan_in' or 'LeCun' initialization. 'Kaiming' initialization is the same up to multiplicative constants.</p> <p>Methods:</p> Name Description <code>bias_init_scale</code> <code>bias_lr_scale</code> <code>weight_init_scale</code> <code>weight_lr_scale</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Standard.bias_init_scale","title":"bias_init_scale","text":"<pre><code>bias_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Standard.bias_lr_scale","title":"bias_lr_scale","text":"<pre><code>bias_lr_scale(\n    fan_in,\n    fan_out,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n)\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Standard.weight_init_scale","title":"weight_init_scale","text":"<pre><code>weight_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Standard.weight_lr_scale","title":"weight_lr_scale","text":"<pre><code>weight_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.NeuralTangent","title":"NeuralTangent","text":"<p>               Bases: <code>Parametrization</code></p> <p>Neural Tangent Parametrization (NTP).</p> <p>The neural tangent parametrization enables the theoretical analysis of the training dynamics of infinite-width neural networks (<code>Jacot et al., 2018</code>_) via the neural tangent kernel. However, NTP does not admit feature learning, as features are effectively fixed at initialization.</p> <p>.. _Jacot et al., 2018: http://arxiv.org/abs/1806.07572</p> <p>Methods:</p> Name Description <code>bias_init_scale</code> <code>bias_lr_scale</code> <code>weight_init_scale</code> <code>weight_lr_scale</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.NeuralTangent.bias_init_scale","title":"bias_init_scale","text":"<pre><code>bias_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.NeuralTangent.bias_lr_scale","title":"bias_lr_scale","text":"<pre><code>bias_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.NeuralTangent.weight_init_scale","title":"weight_init_scale","text":"<pre><code>weight_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.NeuralTangent.weight_lr_scale","title":"weight_lr_scale","text":"<pre><code>weight_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.MaximalUpdate","title":"MaximalUpdate","text":"<p>               Bases: <code>Parametrization</code></p> <p>Maximal update parametrization (\\(\\mu P\\)).</p> <p>The maximal update parametrization (<code>Yang et al., 2021</code>, <code>Yang et al., 2021b</code>) enforces:     - stable training dynamics, meaning (pre)activations and logits at activations are independent       of width at initialization and features and logits do not explode during training.     - feature learning, meaning the model can learn representations from the data at any width.</p> <p>This parametrization is particularly useful when training large models, since it maintains stable training dynamics at any width and enables hyperparameter transfer. This means one can tune the learning rate on a smaller model and achieve good generalization with the tuned learning rate for a large model.</p> <p>.. _Yang et al., 2021: http://arxiv.org/abs/2011.14522 .. _Yang et al., 2021b: http://arxiv.org/abs/2203.03466</p> <p>Methods:</p> Name Description <code>bias_init_scale</code> <code>bias_lr_scale</code> <code>weight_init_scale</code> <code>weight_lr_scale</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.MaximalUpdate.bias_init_scale","title":"bias_init_scale","text":"<pre><code>bias_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.MaximalUpdate.bias_lr_scale","title":"bias_lr_scale","text":"<pre><code>bias_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.MaximalUpdate.weight_init_scale","title":"weight_init_scale","text":"<pre><code>weight_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.MaximalUpdate.weight_lr_scale","title":"weight_lr_scale","text":"<pre><code>weight_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"examples/varibo/classification/classification/","title":"Classification","text":"<p>Coming soon!</p>"},{"location":"examples/varibo/regression/regression/","title":"Regression on Toy Data","text":"<p>This toy example shows how to train a variational neural network via implicit regularization via SGD initialized to the prior as described in this paper. This avoids the computational cost of regularization which for non-trivial variational families is often significant and preserves beneficial inductive biases.</p> <p>You can run this example yourself via the corresponding standalone script.</p>"},{"location":"examples/varibo/regression/regression/#data","title":"Data","text":"<p>We begin by generating some synthetic training and test data.</p> <p></p>"},{"location":"examples/varibo/regression/regression/#model","title":"Model","text":"<p>Next, we define a fully-connected stochastic neural network using a pre-defined <code>models.MLP</code>.</p> Model<pre><code>from torch import nn\n\nfrom inferno import bnn\n\nmodel = bnn.Sequential(\n    inferno.models.MLP(\n        in_size=1,\n        hidden_sizes=[hidden_width] * num_hidden_layers,\n        out_size=1,\n        activation_layer=nn.SiLU,  # (1)!\n        cov=[bnn.params.FactorizedCovariance()]\n        + [None] * (num_hidden_layers - 1)\n        + [bnn.params.FactorizedCovariance()],\n        bias=True,\n    ),\n    nn.Flatten(-2, -1),\n    parametrization=bnn.params.MUP(),\n)\n</code></pre> <ol> <li>PyTorch <code>nn.Module</code>s can be used as part of <code>inferno</code> models.</li> </ol>"},{"location":"examples/varibo/regression/regression/#training","title":"Training","text":"<p>We train the model via the expected loss,  \\( \\bar{\\ell}(\\theta) = \\mathbb{E}_{q_\\theta(w)}[\\ell(y, f_w(X))] \\) i.e. the average loss of the model when drawing weights from the variational distribution \\(q_\\theta(w)\\). In practice, for efficiency we only use a single sample per batch during training.</p> Training<pre><code># Loss function\nloss_fn = nn.MSELoss()\n\n# Optimizer\noptimizer = torch.optim.SGD(\n    params=model.parameters_and_lrs(\n        lr=lr, optimizer=\"SGD\"\n    ),  # Sets module-specific learning rates\n    lr=lr,\n    momentum=0.9,\n)\n\n# Training loop\nfor epoch in tqdm.trange(num_epochs):\n    model.train()\n\n    for X_batch, y_batch in iter(train_dataloader):\n        optimizer.zero_grad()\n\n        X_batch = X_batch.to(device=device)\n        y_batch = y_batch.to(device=device)\n\n        y_batch_pred = model(X_batch)  # Uses a single parameter sample\n\n        loss = loss_fn(y_batch_pred, y_batch)\n\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"examples/varibo/regression/regression/#results","title":"Results","text":""},{"location":"examples/varibo/regression/regression/#learning-curves","title":"Learning Curves","text":""},{"location":"examples/varibo/regression/regression/#prediction","title":"Prediction","text":""}]}